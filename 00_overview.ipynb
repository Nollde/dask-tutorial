{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Dask\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/dask_horizontal.svg\" align=\"right\" width=\"30%\" alt=\"Dask logo\">\n",
    "\n",
    "\n",
    "Wouldn't it be nice if you could keep writing simple Python code and scale it up **from a single node of your laptop to a massive computing cluster**? Welcome to Dask.\n",
    "\n",
    "Dask enables computations on:\n",
    "- Larger-than-memory data\n",
    "- More than one core in parallel\n",
    "- More than one machine in parallel\n",
    "\n",
    "...and all using simple python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is [Dask](\"https://www.dask.org/\")?\n",
    "\n",
    "Today, we will see two main components of Dask:\n",
    "* Dask Collections/API:\n",
    "    * High-level (Arrays & Dataframes)\n",
    "    * Low-level (Delayed & Futures)\n",
    "* Distributed: to create and manage clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collections/API\n",
    "\n",
    "Dask provides **multi-core** and **distributed+parallel** execution on **larger-than-memory** datasets\n",
    "\n",
    "We can think of Dask's APIs (also called collections)  at a high and a low level:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/high_vs_low_level_coll_analogy.png\" width=\"75%\" alt=\"High vs Low level clothes analogy\" style=\"background-color:white;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed\n",
    "\n",
    "Most of the times when you are using Dask, you will be using a distributed scheduler. The Dask cluster is structured as:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/distributed-overview.png\" width=\"75%\" alt=\"Distributed overview\" style=\"background-color:white;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1. You should clone this repository\n",
    "\n",
    "\n",
    "    git clone git@github.com:Nollde/dask-tutorial.git\n",
    "\n",
    "and then install necessary packages.\n",
    "\n",
    "#### 2) Create a conda environment\n",
    "\n",
    "In the main repo directory\n",
    "\n",
    "\n",
    "    conda env create -f env.yml\n",
    "    conda activate dask-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Structure\n",
    "\n",
    "Each section is a Jupyter notebook. There's a mixture of text, code, and exercises.\n",
    "\n",
    "0. [Overview](00_overview.ipynb) - dask's place in the universe.\n",
    "\n",
    "1. [Dataframe](01_dataframe.ipynb) - parallelized operations on many pandas dataframes spread across your cluster.\n",
    "\n",
    "2. [Array](02_array.ipynb) - blocked numpy-like functionality with a collection of numpy arrays spread across your cluster.\n",
    "\n",
    "3. [Delayed](03_delayed.ipynb) - the single-function way to parallelize general python code.\n",
    "\n",
    "5. [Futures](04_futures.ipynb) - non-blocking results that compute asynchronously.\n",
    "\n",
    "4. [Distributed](05_distributed.ipynb) - Dask's scheduler for clusters, with details of how to view the UI.\n",
    "\n",
    "6. Conclusion & Beyond Dask"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
